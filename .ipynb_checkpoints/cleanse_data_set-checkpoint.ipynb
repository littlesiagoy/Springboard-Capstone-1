{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elliot.ting\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "labels ['FIPS'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-db1019759e8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;31m# Change the final cardio dataset columns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m \u001b[0mfinal_cardio_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FIPS'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[0mfinal_cardio_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'STATE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Year'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Mortality Rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[0;32m   2159\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2160\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2161\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2162\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2163\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   3622\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3623\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[1;32m-> 3624\u001b[1;33m                                  labels[mask])\n\u001b[0m\u001b[0;32m   3625\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3626\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: labels ['FIPS'] not contained in axis"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "'''Data Cleansing '''\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "''' Import/Clean the poverty dataset. '''\n",
    "#######################################################################################################################\n",
    "poverty_dataset = pd.read_excel('poverty_rate_dataset.xls', skiprows=[0, 1, 2, 3], header=0)\n",
    "poverty_dataset = poverty_dataset[['STATE', 'Total', 'Number', 'Percent']]\n",
    "\n",
    "\n",
    "# This will give us the rows where each year starts.\n",
    "test = poverty_dataset[poverty_dataset['Total'].isnull()]\n",
    "\n",
    "# This is the list of years we want to pull out.\n",
    "list_of_years = [1980, 1985, 1990, 1995, 2000, 2005, 2010]\n",
    "\n",
    "# year index + 2, next year is the formula for the indices in the poverty dataset\n",
    "data_1980 = poverty_dataset.iloc[1961:2012]\n",
    "data_1985 = poverty_dataset.iloc[1696:1747]\n",
    "data_1990 = poverty_dataset.iloc[1431:1482]\n",
    "data_1995 = poverty_dataset.iloc[1166:1217]\n",
    "data_2000 = poverty_dataset.iloc[901:952]\n",
    "data_2005 = poverty_dataset.iloc[636:687]\n",
    "data_2010 = poverty_dataset.iloc[371:422]\n",
    "\n",
    "# Add a year column to the data.\n",
    "data_sets = [data_1980, data_1985, data_1990, data_1995, data_2000, data_2005, data_2010]\n",
    "years = [1980, 1985, 1990, 1995, 2000, 2005, 2010]\n",
    "\n",
    "for index in range(len(data_sets)):\n",
    "    data_sets[index]['Year'] = years[index]\n",
    "\n",
    "# The result is the cleansed poverty_dataset.\n",
    "final_poverty_dataset = pd.concat(data_sets)\n",
    "\n",
    "# The columns are STATE, Total, Number, Percent, Year\n",
    "# Convert the total number, and percent columns to floats.\n",
    "final_poverty_dataset.Total = final_poverty_dataset.Total.apply(pd.to_numeric)\n",
    "final_poverty_dataset.Number = final_poverty_dataset.Number.apply(pd.to_numeric)\n",
    "final_poverty_dataset.Percent = final_poverty_dataset.Percent.apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "''' Import/Clean the Cardio Dataset. '''\n",
    "########################################################################################################################\n",
    "# Import the cardio datset.\n",
    "cardio_dataset = pd.read_excel('cardiovascular_dataset.xlsx', sheet_name='Cardiovascular diseases', skiprows=[0])\n",
    "\n",
    "# Drop the 2014 column and the # change in mortality rate column.\n",
    "cardio_dataset = cardio_dataset.drop(['Mortality Rate, 2014*', '% Change in Mortality Rate, 1980-2014'], axis=1)\n",
    "\n",
    "# After inspecting the codebook, we see that all the states will be given an unique FIPS value.  It ranges from 1 to 56.\n",
    "# Initialize empty dataset.\n",
    "final_cardio_dataset = pd.DataFrame()\n",
    "\n",
    "# Return the row for each state and append it to the empty dataset.\n",
    "for index in range(57):\n",
    "    final_cardio_dataset = final_cardio_dataset.append(cardio_dataset[cardio_dataset.FIPS == index])\n",
    "\n",
    "final_cardio_dataset = pd.melt(final_cardio_dataset, id_vars=['Location', 'FIPS'])\n",
    "\n",
    "\n",
    "final_cardio_dataset.columns = [['Location', 'FIPS', 'Year', 'Mortality Rate']]\n",
    "\n",
    "# Replace the values in the variable column\n",
    "final_cardio_dataset.replace('^Mortality Rate, ', '', regex=True, inplace=True)\n",
    "final_cardio_dataset.replace('\\*$', '', regex=True, inplace=True)\n",
    "\n",
    "# Get rid of the confidence interval in the mortality rate column.\n",
    "final_cardio_dataset['Mortality Rate'] = final_cardio_dataset['Mortality Rate'].replace(' .*', '', regex=True)\n",
    "\n",
    "# Convert all the columns except for the states into numeric.\n",
    "final_cardio_dataset.Year = final_cardio_dataset.Year.apply(pd.to_numeric)\n",
    "final_cardio_dataset['Mortality Rate'] = final_cardio_dataset['Mortality Rate'].apply(pd.to_numeric)\n",
    "\n",
    "# Replace all District of Columbia values in cardio dataset to D.C.\n",
    "final_cardio_dataset.replace('District of Columbia', 'D.C.', regex=True, inplace=True)\n",
    "\n",
    "final_cardio_dataset.drop('FIPS', axis=1, inplace=True)\n",
    "\n",
    "# Rename the final poverty dataset columns.\n",
    "final_poverty_dataset.columns = ['STATE', 'Total (1000s)', 'Number (1000s)', 'Percent', 'Year']\n",
    "\n",
    "\n",
    "''' Import/Clean the US GDP dataset. '''\n",
    "#######################################################################################################################\n",
    "us_gdp = pd.read_csv('US GDP 1980-2017.csv', skiprows=[0, 1, 2, 3], index_col=0)\n",
    "\n",
    "# Pull out only the US GDP for each year.\n",
    "us_gdp = us_gdp.drop([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26])\n",
    "\n",
    "# Turn from the wide format to short format.\n",
    "us_gdp = pd.melt(us_gdp)\n",
    "\n",
    "# rename columns and drop the first row.\n",
    "us_gdp.columns = ['Year', 'Gross Domestic Product ($)']\n",
    "us_gdp = us_gdp.drop([0])\n",
    "\n",
    "# Convert column from str to int.\n",
    "us_gdp['Year'] = us_gdp['Year'].astype(int)\n",
    "\n",
    "# Set the index so we can grab the years we want.\n",
    "us_gdp = us_gdp.set_index('Year')\n",
    "us_gdp = us_gdp.loc[years]\n",
    "\n",
    "# Multiply the GDp to change the scale from billions of dollars to just dollars.\n",
    "us_gdp.loc[:, 'Gross Domestic Product ($)'] = us_gdp.loc[:, 'Gross Domestic Product ($)'] * 1000000000\n",
    "\n",
    "\n",
    "''' Import/Clean the US Productivity dataset. '''\n",
    "#######################################################################################################################\n",
    "# Import the dataset.\n",
    "productivity = pd.read_csv('US Overall Labour Productivity 1970-2016.csv')\n",
    "\n",
    "# We want the measures of productivity to be in USD.\n",
    "productivity = productivity.loc[productivity.loc[:, 'MEASURE'] == 'USD']\n",
    "\n",
    "# We only want the Time and value columns.\n",
    "productivity = productivity[['TIME', 'Value']]\n",
    "\n",
    "# Rename the columns to make it more clear.\n",
    "productivity.columns = ['Year', 'GDP Per Hour Worked']\n",
    "\n",
    "productivity = productivity.set_index('Year')\n",
    "\n",
    "# We want specific set of years.\n",
    "productivity = productivity.loc[years]\n",
    "\n",
    "\n",
    "''' Import/Clean the State Unemployment Rate dataset. '''\n",
    "#######################################################################################################################\n",
    "state_unemploymnet = pd.read_excel('US State Unemployment Rates 1980-2015.xls', sheet_name='States', skiprows=[0, 1, 2, 3, 4])\n",
    "\n",
    "# Get rid of the unncessary column.\n",
    "state_unemploymnet = state_unemploymnet.drop('Fips', axis=1)\n",
    "\n",
    "# Get melt the data from wide to long.\n",
    "state_unemploymnet = pd.melt(state_unemploymnet, id_vars=['Area'])\n",
    "\n",
    "# Replace the District of Columbia value to D.C.\n",
    "state_unemploymnet.replace('District of Columbia', 'D.C.', regex=True, inplace=True)\n",
    "\n",
    "state_unemploymnet.columns = ['STATE', 'Year', 'Unemployment Rate']\n",
    "\n",
    "\n",
    "''' Import/Clean the Corruption Dataset. '''\n",
    "#######################################################################################################################\n",
    "# Import the data.\n",
    "corruption = pd.read_excel('final-transparency-indices-scores-september-2014.xlsx', sheet_name='ATI, ITI AND TI SCORES')\n",
    "\n",
    "# Pull out the United States data.\n",
    "corruption = corruption.loc[corruption.loc[:, 'Country'] == 'United States']\n",
    "\n",
    "# Pull out only the years we want.\n",
    "corruption = corruption.set_index('Year')\n",
    "corruption = corruption.loc[years]\n",
    "\n",
    "# Drop the extra columns.\n",
    "corruption = corruption[['Information Transparency Score', 'Accountability Transparency Score', 'Transparency Index Score']]\n",
    "\n",
    "\n",
    "''' Combine the datasets. '''\n",
    "#######################################################################################################################\n",
    "# Combine the overall US data.\n",
    "us_dataset = us_gdp.join([corruption, productivity])\n",
    "\n",
    "# Create a State column with just the name United States.\n",
    "us_dataset['STATE'] = 'United States'\n",
    "\n",
    "# Combine the state level data.\n",
    "states_dataset = state_unemploymnet.merge(final_poverty_dataset)\n",
    "\n",
    "# Change the final cardio dataset columns.\n",
    "final_cardio_dataset.drop('FIPS', axis=1, inplace=True)\n",
    "final_cardio_dataset.columns = ['STATE', 'Year', 'Mortality Rate']\n",
    "\n",
    "# Merge the cardio dataset with the state dataset.\n",
    "states_dataset = final_cardio_dataset.merge(states_dataset)\n",
    "\n",
    "\n",
    "''' Impute the overall US level data into the state dataset. '''\n",
    "########################################################################################################################\n",
    "states_dataset['Gross Domestic Product ($)'] = ''\n",
    "states_dataset['Information Transparency Score'] = ''\n",
    "states_dataset['Accountability Transparency Score'] = ''\n",
    "states_dataset['Transparency Index Score'] = ''\n",
    "states_dataset['GDP Per Hour Worked'] = ''\n",
    "\n",
    "for value in years:\n",
    "    states_dataset.loc[states_dataset.loc[:, 'Year'] == value, 'Gross Domestic Product ($)'] = us_dataset.loc[value, 'Gross Domestic Product ($)']\n",
    "    states_dataset.loc[states_dataset.loc[:, 'Year'] == value, 'Information Transparency Score'] = us_dataset.loc[value, 'Information Transparency Score']\n",
    "    states_dataset.loc[states_dataset.loc[:, 'Year'] == value, 'Accountability Transparency Score'] = us_dataset.loc[value, 'Accountability Transparency Score']\n",
    "    states_dataset.loc[states_dataset.loc[:, 'Year'] == value, 'Transparency Index Score'] = us_dataset.loc[value, 'Transparency Index Score']\n",
    "    states_dataset.loc[states_dataset.loc[:, 'Year'] == value, 'GDP Per Hour Worked'] = us_dataset.loc[value, 'GDP Per Hour Worked']\n",
    "\n",
    "\n",
    "# The state dataset is now the final master dataset.\n",
    "master_dataset = states_dataset\n",
    "\n",
    "master_dataset.to_excel('master_dataset.xlsx', index=False)\n",
    "\n",
    "\n",
    "''' Smoothing out the data due to different unit of measurements. '''\n",
    "########################################################################################################################\n",
    "# Due to the different units of measure between the variables, preprocessing is needed.\n",
    "\n",
    "# Take out our target variable of mortality rates before we perform the preprocessing.\n",
    "y = master_dataset['Mortality Rate']\n",
    "\n",
    "# Take out the STATE, year, and mortality Rate variables for ease of interpretation after preprocessing.\n",
    "samples = master_dataset.drop(['STATE', 'Year', 'Mortality Rate'], axis=1)\n",
    "\n",
    "# Scale the with the RobustScaler, since the number of outliers is unknown.\n",
    "data = RobustScaler().fit_transform(samples)\n",
    "data = pd.DataFrame(data, columns=samples.columns)\n",
    "\n",
    "# Add the state and year data back.\n",
    "data['STATE'] = master_dataset['STATE']\n",
    "data['Year'] = master_dataset['Year']\n",
    "\n",
    "# Reorgnize the columns.\n",
    "data = data[['STATE', 'Year', 'Unemployment Rate', 'Total (1000s)',\n",
    "                'Number (1000s)', 'Percent', 'Gross Domestic Product ($)',\n",
    "                'Information Transparency Score', 'Accountability Transparency Score',\n",
    "                'Transparency Index Score', 'GDP Per Hour Worked']]\n",
    "\n",
    "# Change the value of the state into a number.\n",
    "data_set = sorted(set(data.STATE))\n",
    "for index, value in enumerate(data_set):\n",
    "    data.loc[data.loc[:, 'STATE'] == value, 'STATE'] = index\n",
    "\n",
    "\n",
    "''' Exploratory Data Analysis '''\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "''' Outlier detection with LOF '''\n",
    "########################################################################################################################\n",
    "# Initialize the model.\n",
    "lof = LocalOutlierFactor()\n",
    "\n",
    "# Calculate the scores. (1 is fine, -1 is an outlier.)\n",
    "outlier_scores = pd.DataFrame(lof.fit_predict(data))\n",
    "\n",
    "# Append the score to the dataset.\n",
    "data['Local Outlier Factor'] = outlier_scores\n",
    "\n",
    "# We have a total of 36 outliers.\n",
    "outliers = data.loc[data.loc[:, 'Local Outlier Factor'] == -1]\n",
    "\n",
    "# Convert the state numbers back into names by first creating the dictionary of values.\n",
    "state_names = {}\n",
    "for index, value in enumerate(data_set):\n",
    "    state_names[index] = value\n",
    "\n",
    "# Now convert the state numbers back into names using the dictionary.\n",
    "for value in list(set(outliers.STATE)):\n",
    "    outliers.STATE.replace(value, state_names[value], inplace=True)\n",
    "\n",
    "# Print out all the states with outliers.\n",
    "print(outliers.STATE.value_counts())\n",
    "\n",
    "# Remove the outliers from the dataset.\n",
    "data_with_outliers = data.drop('Local Outlier Factor', axis=1)\n",
    "data_no_outliers = data.loc[data.loc[:, 'Local Outlier Factor'] == 1].drop('Local Outlier Factor', axis=1)\n",
    "\n",
    "\n",
    "''' Pairwise Correlations '''\n",
    "#######################################################################################################################\n",
    "# Calculate all the pairwise correlations\n",
    "corr1 = data_no_outliers.corr()\n",
    "corr2 = data_with_outliers.corr()\n",
    "\n",
    "# Heat map of the different correlations.\n",
    "sns.heatmap(corr1)\n",
    "sns.heatmap(corr2)\n",
    "\n",
    "# The statistical significance of each pairwise correlation.\n",
    "correlations_no_outliers = {}\n",
    "correlations_with_outliers = {}\n",
    "\n",
    "columns = data_no_outliers.columns.tolist()\n",
    "column2 = data_with_outliers.columns.tolist()\n",
    "\n",
    "for a, b in itertools.combinations(columns, 2):\n",
    "    correlations_no_outliers[a + '->' + b] = pearsonr(data_no_outliers.loc[:, a], data_no_outliers.loc[:, b])\n",
    "for a, b in itertools.combinations(column2, 2):\n",
    "    correlations_with_outliers[a + '->' + b] = pearsonr(data_with_outliers.loc[:, a], data_with_outliers.loc[:, b])\n",
    "\n",
    "result_no_outliers = pd.DataFrame.from_dict(correlations_no_outliers, orient='index')\n",
    "result_with_outliers = pd.DataFrame.from_dict(correlations_with_outliers, orient='index')\n",
    "\n",
    "result_no_outliers.columns = ['Pearson Correlation', 'Statistical_Significance']\n",
    "result_with_outliers.columns = ['Pearson Correlation', 'Statistical_Significance']\n",
    "\n",
    "result_no_outliers = result_no_outliers.drop('Pearson Correlation', axis=1)\n",
    "result_with_outliers = result_with_outliers.drop('Pearson Correlation', axis=1)\n",
    "\n",
    "# Print the heatmaps.\n",
    "sns.heatmap(result_no_outliers)\n",
    "sns.heatmap(result_with_outliers)\n",
    "\n",
    "\n",
    "''' Dimensionality Reduction '''\n",
    "########################################################################################################################\n",
    "# Initialize the PCA model.\n",
    "pca = PCA()\n",
    "\n",
    "# Perform the PCA\n",
    "pca.fit_transform(data_no_outliers)\n",
    "\n",
    "# Plot the results.\n",
    "plt.plot(pca.explained_variance_)\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained variance')\n",
    "plt.title('Explained Variance by Variable')\n",
    "\n",
    "# Print out the explained variance\n",
    "print(pd.DataFrame(pca.explained_variance_ratio_, columns=['Explained Variance Ratio'], index=data_no_outliers.columns))\n",
    "\n",
    "# Test a linear regression using all the data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
